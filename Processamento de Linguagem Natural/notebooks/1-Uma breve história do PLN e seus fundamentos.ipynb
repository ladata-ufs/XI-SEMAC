{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a>\n",
        "    <img src=\"https://ladata-ufs.github.io/Assets/LOGO-EXTENSO-CLARA.png\" width=175>\n",
        "</a>\n",
        "\n",
        "# **Processamento de Linguagem Natural: das abordagens clássicas aos LLMs**  \n",
        "> Túlio Gois; Ana Laylla Rocha; Daivik Celeste; David Henrique Ferraz; Letícia Santos."
      ],
      "metadata": {
        "id": "mHdsw42X-OMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Dia 1: Uma breve história do PLN e seus fundamentos**"
      ],
      "metadata": {
        "id": "c-iJwCgdM01O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dependências**"
      ],
      "metadata": {
        "id": "ryNWtmItiAZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ipWi4Ml-Bwu"
      },
      "outputs": [],
      "source": [
        "#instalações\n",
        "!pip install --quiet nltk spacy enelvo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import nltk\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "d5uupp0niEyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "spacy.cli.download(\"pt_core_news_md\")\n",
        "nlp = spacy.load(\"pt_core_news_md\")"
      ],
      "metadata": {
        "id": "takc_6aSELDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A unidade de processamento**\n",
        "É importante deixar claro o que vamos processar quanto tratamos de processamento de linguagem natural, ou seja, qual o elemento que vai ser a menor unidade para os métodos computacionais. De modo geral, os modelos de PLN tratam as **palavras** como unidades primárias de processamento.\n",
        "\n",
        "> **Atenção**  \n",
        "> Não há um consenso entre pesquisadores e profissionais da área sobre essa delimitação. Nem mesmo na linguística conseguimos encontrar essa certeza, e isso fica claro por conta de como cada subárea dos estudos linguísticos entendem suas unidades mínimas de processamento: por exemplo, para a fonologia, a menor unidade seria o fonema, já para a morfologia, seria o fonema.\n",
        "\n",
        "Além disso, é importante definir o que seria palavra. Para isso, vamos adotar a explicação presente no livro \"Processamento de Linguagem Natural: Conceitos, Técnicas e Aplicações em Português\":  \n",
        "> *Uma palavra é uma unidade grafológica delimitada, nas línguas europeias, entre espaços em branco na representação gráfica, ou entre um espaço em branco e um sinal de pontuação.*   \n",
        "\n"
      ],
      "metadata": {
        "id": "5xFxoG4aujvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Processamento morfológico**\n",
        "Na maioria das aplicações de PLN vamos encontrar algumas etapas em comum, que aparecem com o nome de pré-processamento. As mais comuns tratam da \"quebra\" de um texto, seja em sentenças (**sentenciação**) ou palavras (**tokenização**), e normalização (como **lematização** ou radicalização).\n",
        "\n",
        "Além do pré-processamento, podemos encontrar etapas de processamento, como a **etiquetagem morfossintática** (PoS tagging) — para extrair a classe gramatical de uma palavra — e a **anotação de atributos morfológicos**."
      ],
      "metadata": {
        "id": "nCdNKmlkiKkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Token e Type**\n",
        "***Token* é um termo que significa qualquer sequência de caracteres que tem valor**. Ao olharmos para o português, podemos dizer que em uma determinada sentença, o número de tokens é representado pela `quantidade de palavras + sinais de pontuação`.\n",
        "\n",
        "A tarefa que trata de separar as unidades linguísticas mínimas (tokens) é denominada **tokenização** e, no português, ela ocorre através da separação de palavras por delimitadores (sinais de pontuação ou espaços em branco).\n",
        "\n",
        "Já o ***type***, refere-se aos **tokens únicos** encontrados em uma frase ou texto. Ou seja, dada uma frase onde uma palavra aparece $n$ vezes como token, ela aparecerá apenas 1 vez como type.\n"
      ],
      "metadata": {
        "id": "y2lkjIPWG8pU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Tokenização**"
      ],
      "metadata": {
        "id": "uHn-xfc8YNUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#frase de exemplo\n",
        "frase = \"Boas vindas ao minicurso de Processamento de Linguagem Natural!\""
      ],
      "metadata": {
        "id": "bvubbcP_G-Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokenização simples (usando o split)\\n\")\n",
        "print(frase.split())"
      ],
      "metadata": {
        "id": "C-qSIPcfpOG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokenização com a Natural Language Toolkit (NLTK)\\n\")\n",
        "print(nltk.tokenize.word_tokenize(frase))"
      ],
      "metadata": {
        "id": "eo8vVdP_x9RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(frase)\n",
        "\n",
        "print(\"Tokenização com o spaCy\\n\")\n",
        "print([token.text for token in doc])"
      ],
      "metadata": {
        "id": "76FZg8OW3hBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Verificando os types**\n",
        "Aqui, vamos utilizar a função `set` do Python, que cria um conjunto não ordenado de itens únicos. Como estamos aplicando essa função à uma lista de tokens, será gerado um conjunto com as palavras únicas (types).\n"
      ],
      "metadata": {
        "id": "Q-gBs6QGsTtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.tokenize.word_tokenize(frase)\n",
        "types = set(tokens)\n",
        "print(types)"
      ],
      "metadata": {
        "id": "eUxEA7jVsgjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Lematização**\n",
        "A lematização é uma tarefa comumente usada como normalização de palavras para PLN. Ela consiste, de modo geral, em converter uma palavra para o seu formato presente no dicionário. É importante notar que isso requer conhecimento das propriedades sintático-semânticas das palavras. Por exemplo:\n",
        "> O lema das palavras \"filósofos\" e \"filósofa\" é \"**filósofo**\";  \n",
        "> Já o lema das palavras \"filosofei\" e \"filosofamos\" é \"**filosofar**."
      ],
      "metadata": {
        "id": "aoimeFdRHtaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teste_lema = \"Vamos fazer um teste, testando o lematizador do spacy e vendo quais lemas saem\"\n",
        "doc = nlp(teste_lema)\n",
        "\n",
        "print(\"Lematização com o spaCy\\n\")\n",
        "print([(token.text, token.lemma_) for token in doc])"
      ],
      "metadata": {
        "id": "VPeybLqOHtKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Léxico**\n",
        "Quando estamos tratando do conjunto que envolve as palavras de uma língua, juntamente com suas definições morfossintáticas, falamos em **léxico**. Geralmente cada palavra do léxico tem uma ou mais triplas contendo sua categoria gramatical (Part-of-Speech), seu lema e suas características morfológicas."
      ],
      "metadata": {
        "id": "XDJ0NBhcsizJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PoS tagging**\n",
        "O PoS tagging (ou etiquetagem morfossintática) é a técnica que envolve a atribuição das etiquetas gramaticais de cada palavra em um texto. Essas classes de palavras são universais e valem para a maioria das línguas.\n",
        "\n",
        "Vamos relembrá-las: substantivos, verbos, adjetivos, advérbios, pronomes, numerais, artigos, conjunções, preposições e interjeições.\n",
        "\n",
        "É importante notar que para uma etiquetagem morfossintática ser feita corretamente, analisar o contexto é essencial, pois existem palavras polissêmicas, ambíguas, homônimas etc. Por exemplo:\n",
        "> \"Hoje eu fui para o trabalho\", onde *trabalho* é um substantivo.  \n",
        "> \"Sexta é o dia que eu mais trabalho\", onde *trabalho* é um verbo."
      ],
      "metadata": {
        "id": "n7MKV2gSiPIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(frase)\n",
        "\n",
        "print(\"PoS tagging com o spaCy\\n\")\n",
        "print([(token.text, token.pos_) for token in doc])"
      ],
      "metadata": {
        "id": "__KBTZmBD0GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.tokenize.word_tokenize(frase)\n",
        "tags = nltk.pos_tag(tokens)\n",
        "\n",
        "print([tag for tag in tags])"
      ],
      "metadata": {
        "id": "dYAvgs0XHoqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Atributos morfológicos**\n",
        "Os atributos morfológicos (também chamados de *features* ou *feats*) tratam de informações sobre as características gramaticais das palavras em um texto. Esses atributos vão apontar características como número, gênero, grau, modo, tempo, entre outras."
      ],
      "metadata": {
        "id": "Zl5x8N6hJJo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(frase)\n",
        "\n",
        "dados_morph = []\n",
        "for token in doc:\n",
        "    dados_morph.append({\n",
        "        \"token\": token.text,\n",
        "        \"morfologia\": token.morph.to_dict()\n",
        "    })\n",
        "\n",
        "df_morph = pd.DataFrame(dados_morph)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "display(df_morph)"
      ],
      "metadata": {
        "id": "jgmEAqChJR2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Juntando tudo**\n",
        "Vamos ver como fica a frase processada com todas as técnicas que vimos até aqui."
      ],
      "metadata": {
        "id": "WCiUpCy-JZZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exemplo = \"Vamos colocar uma frase legal aqui.\"\n",
        "\n",
        "doc = nlp(exemplo)\n",
        "\n",
        "morph = []\n",
        "for token in doc:\n",
        "    morph.append({\n",
        "        \"token\": token,\n",
        "        \"lema\": token.lemma_,\n",
        "        \"pos\": token.pos_,\n",
        "        \"morfologia\": token.morph.to_dict()\n",
        "    })\n",
        "\n",
        "df_morph = pd.DataFrame(morph)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "display(df_morph)"
      ],
      "metadata": {
        "id": "kg1RLeuLJcSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sintaxe: a ordem e função das palavras**\n",
        "Ao olharmos para a **forma como as palavras ficam ordenadas em uma determinada frase e as funções que elas assumem nessas posições**, estamos estudando a **sintaxe**. É através da sintaxe que reconhecemos as regras que ditam quais agrupamentos de palavras fazem sentido ou não.\n",
        "\n",
        "Na análise sintática, estamos trabalhando no nível das orações, ou seja, um conjunto de palavras que tratam de algum evento do mundo. Geralmente, as palavras de uma oração conseguem responder às perguntas: **\"quem?\", faz \"o quê?\", \"para quem?\", \"como?\", \"quando?\",** entre outras.\n",
        "\n",
        "É importante lembrarmos que **em uma sentença** (que começa com letra maiúscula e termina com um sinal de pontuação) **pode haver mais de uma oração**."
      ],
      "metadata": {
        "id": "F7nwhHdpJxYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Parsing e parsers***\n",
        "\n",
        "O processo de analisar essa estruturação de orações em PLN é o *parsing*. Essa tarefa consiste em, dada uma entrada (uma sentença), um modelo vai prever a estrutura sintática dessa entrada, identificando as unidades na sentença e estabelecendo suas relações gramaticais.\n",
        "\n",
        "As ferramentas que realizam essa tarefa de *parsing* são chamadas de *parsers*."
      ],
      "metadata": {
        "id": "ldGKCq4jxshL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Hoje estamos apresentando o dia um do minicurso de PLN e amanhã apresentaremos o dia dois\")\n",
        "\n",
        "print(\"Testando o parser do spaCy\\n\")\n",
        "displacy.render(doc, style='dep', jupyter=True)\n"
      ],
      "metadata": {
        "id": "We_XtoK-yJVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Corpora**\n",
        "Um dos recursos mais importantes para o processamento linguístico é um *corpus* anotado, também chamado de *treebank*, isto é, textos anotados com suas etiquetas de classes de palavras e relações sináticas.\n",
        "\n",
        "> ***Corpus*** é o nome dado à um conjunto de dados linguísticos composto por textos escritos ou transcrições de fala. *Corpora* é o plural de *corpus*.\n",
        "\n",
        "Um *corpus* em português muito utilizado no treinamento de modelos de análise sintáticas é o [Bosque](https://www.linguateca.pt/Floresta/corpus.html). O Bosque faz parte de um *corpus* ainda maior, chamado [Floresta Sintá(c)tica\n",
        "](https://www.linguateca.pt/Floresta/principal.html), que é composto por outros *subcorpora*."
      ],
      "metadata": {
        "id": "BcCFtNWFxsL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Outros recursos de PLN para português**\n",
        "\n",
        "* [**Enelvo**](https://thalesbertaglia.com/enelvo/o): biblioteca Python para normalização de textos que permite a correção de abreviações, gírias, erros ortográficos, entre outras funcinoalidades;\n",
        "* [**Stanza**](https://stanfordnlp.github.io/stanza/): biblioteca Python criada pelo Stanford NLP Group que oferece uma pipeline neural completa para várias tarefas de processamento de linguagem, como tokenização, etiquetagem morfossintática, lematização e análise de dependências;\n",
        "* [**Porttagger**](https://huggingface.co/spaces/Emanuel/porttagger): etiquetador morfossintático para PT-BR. Utiliza as etiquetas do modelo gramatical [Universal Dependencies](https://universaldependencies.org) e realiza a classificação com base no BERTimbau, modelo de linguagem baseado em transformers. Atualmente é o estado da arte de PoS Tagging para o português;\n",
        "* [**Portparser**](https://github.com/LuceleneL/Portparser): modelo de parsing para PT-BR. Faz anotação de Part-of-Speech (PoS), identificação de lemas, extração de características morfológicas e\n",
        "anotação de relações de dependência.\n"
      ],
      "metadata": {
        "id": "noLtgOnDwXT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Referências**\n",
        "* **[Processamento de Linguagem Natural: Conceitos, Técnicas e Aplicações em Português (3ª Ed.)](https://brasileiraspln.com/livro-pln/3a-edicao/)**  \n",
        "* [![spaCy](https://img.shields.io/badge/spaCy-Documentation-09a3d5?style=flat&logo=spacy)](https://spacy.io/) — Documentação oficial do spaCy.  \n",
        "* [![NLTK](https://img.shields.io/badge/NLTK-Documentation-blue?style=flat)](https://www.nltk.org/) — Documentação oficial do NLTK (Natural Language Toolkit)."
      ],
      "metadata": {
        "id": "SXB01L4x32JB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "#### **Acompanhe a LADATA**  \n",
        "[![Instagram](https://img.shields.io/badge/Instagram-%40ladata.ufs-E4405F?style=for-the-badge&logo=instagram&logoColor=white)](https://www.instagram.com/ladata.ufs/)  \n",
        "[![GitHub](https://img.shields.io/badge/GitHub-ladata--ufs-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/ladata-ufs)  \n",
        "[![LinkedIn](https://img.shields.io/badge/LinkedIn-LADATA_UFS-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://br.linkedin.com/company/ladata-ufs)  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "31dPt2VJibUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>\n",
        "Processamento de Linguagem Natural: das abordagens clássicas aos LLMs | XI SEMAC - 2025 <br>\n",
        "<strong>Liga Acadêmica de Ciência de Dados | LADATA</strong>\n",
        "</small>"
      ],
      "metadata": {
        "id": "GfRDv-bTkSbY"
      }
    }
  ]
}